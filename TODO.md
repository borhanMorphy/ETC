### Modules

- [x] Relative Position Encoding
- [x] Global-Local Attention
  - [x] G2G
  - [x] G2L
  - [x] L2G
  - [x] L2L

## Pre Training Objectives

- [ ] MLM
- [ ] CPC

### Attention Types

- [x] Sliding Window Attention
- [x] Full Attention
- [x] Masked Full Attention
- [x] Fast Sliding Window Attention

### Projection Layers

- [x] Separate
- [-] Shared

### Global Input

- [x] Fixed Ratio
- [ ] Fixed Size
- [ ] Structural

### Extra

- [ ] Compare with vanilla transformer
  - [x] Memory Comparison
  - [ ] Speed Comparison <-
  - [ ] Size Comparison <-
